{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import math as m\n",
    "import sklearn.cluster as skc\n",
    "\n",
    "key_iris = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2,\n",
    "            0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into a training and testing set\n",
    "D: dataset given as input\n",
    "percent: the percentage of data used for testing\n",
    "Return: a tupple containing both the training and testing data\n",
    "\"\"\"\n",
    "def split(D, percent):\n",
    "    used = []   # Track indeces used for test set\n",
    "    test = []   # List of vectors for test set\n",
    "    train = []  # List of vectors for training set\n",
    "\n",
    "    # Add random elements into the training set until a threshold\n",
    "    while len(test) < len(D) * percent:\n",
    "        i = rd.randint(0, len(D) - 1)\n",
    "        if i not in used:\n",
    "            used.append(i)\n",
    "            test.append(D.iloc[i].values.tolist())\n",
    "\n",
    "    # Add all remaining elements into the training set\n",
    "    for i in range(len(D)):\n",
    "        if i not in used:\n",
    "            train.append(D.iloc[i].values.tolist())\n",
    "    return (train, test)\n",
    "\n",
    "\"\"\"\n",
    "Calculates Eclidian distance of two lists of floats\n",
    "p1: first vector\n",
    "p2: second vector\n",
    "Return: calculated Euclidian distance\n",
    "\"\"\"\n",
    "def dist_euc(p1, p2):\n",
    "    dist = 0\n",
    "    # Compares the length of p1 and p2 and returns an error if uneven\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        # Iterate over each featur finding (f_p1 - f_p2)^2\n",
    "        for i in range(len(p1)):\n",
    "            dist += (p1[i] - p2[i]) ** 2\n",
    "        # Take the square root of the sum and return\n",
    "        return m.sqrt(dist)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculates Manhattan distance of two lists of floats\n",
    "p1: first vector\n",
    "p2: second vector\n",
    "Return: calculated Manhattan distance\n",
    "\"\"\"\n",
    "def dist_man(p1, p2):\n",
    "    dist = 0\n",
    "    # Compares the length of p1 and p2 and returns an error if uneven\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        for i in range(len(p1)):\n",
    "            dist += abs(p1[i] - p2[i])\n",
    "        return dist\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculates Cosine similarity of two lists of floats\n",
    "p1: first vector\n",
    "p2: second vector\n",
    "Return: calculated Cosine similarity\n",
    "\"\"\"\n",
    "def dist_cos(p1, p2):\n",
    "    dist = 0\n",
    "    # Compares the length of p1 and p2 and returns an error if uneven\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        l = len(p1)\n",
    "        # Takes the dot product of the two vectors\n",
    "        dot = sum(i[0] * i[1] for i in zip(p1, p2))\n",
    "        return dot / (dist_euc([0] * l, p1) * dist_euc([0] * l, p2))\n",
    "\n",
    "train, test = split(pd.read_csv('iris.csv'), 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6\\\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\\\n",
    "    1) Divide your data into random train and test sets\\\n",
    "    2) Training: for a specific label, find the average of each feature. (In case of mnist, this is pixel wise).  give your averages. In case of mint, give this as 10 images, one for each label\\\n",
    "    3) For each data point in the training set, find the closest from the average templates. Use Manhattan, Euclidean and Cosine distances (note that higher cosine value means lower distance). Give the point that label. \\\n",
    "    4) Do the same for the test set\\\n",
    "    5) Give the confusion matrix for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs the algoritm needed to complete problem 6 for the iris dataset\n",
    "train: the training set used in the algorithm\n",
    "test: the testing set used to validate results\n",
    "key: a dictionary to convert from text labels to integer representation and back\n",
    "avg: a dictonary used to keep track of average values for each label in the data\n",
    "\"\"\"\n",
    "def P6Iris(train, test, key, avg):\n",
    "    # Separate the data by label and take the average of each\n",
    "    for row in train:\n",
    "        # Initialize each label with the first occurrance\n",
    "        if avg[row[4]] == 0:\n",
    "            avg[row[4]] = [row[0:4], 1]\n",
    "        else:\n",
    "            avg[row[4]][1] += 1\n",
    "            # Take a rolling average for each feature\n",
    "            for i in range(len(row) - 1):\n",
    "                avg[row[4]][0][i] = avg[row[4]][0][i] + (row[i] - avg[row[4]][0][i]) / avg[row[4]][1]\n",
    "\n",
    "    # Define lists of closest values\n",
    "    train_vals = []\n",
    "    test_vals = []\n",
    "    # Find the closest points to each label using the 3 measurement methods\n",
    "    for val in avg.values():\n",
    "        train_vals.append(find_min_dist(val[0], train))\n",
    "        test_vals.append(find_min_dist(val[0], test))\n",
    "\n",
    "    # Define confusion matrices\n",
    "    conf_train = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "    conf_test  = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "\n",
    "    # Populate confusion matrices\n",
    "    for i in range(len(avg)):\n",
    "        for j in range(len(avg)):\n",
    "            conf_train[i][key[train_vals[i][j]]] += 1\n",
    "            conf_test[i][key[test_vals[i][j]]] += 1\n",
    "\n",
    "    # Print data relavant to the problem\n",
    "    print(avg)\n",
    "    print(train_vals)\n",
    "    print(conf_train)\n",
    "    print(test_vals)\n",
    "    print(conf_test)\n",
    "\n",
    "\"\"\"\n",
    "Finds the minimum distance between an average point and the dataset using\n",
    "    Euclidian distance, Manhattan distance, and Cosine similarity\n",
    "avg: the average values for a given label\n",
    "data: the raw dataset being tested against\n",
    "Return: a set of points representing minumum distance for each distance measure\n",
    "\"\"\"\n",
    "def find_min_dist(avg, data):\n",
    "    # Define maximum possible distances (0 in case of cosine similarity)\n",
    "    dist = [m.inf, m.inf, 0]\n",
    "    keys = [[], [], []]\n",
    "    # Iterate through the dataset and find min distances\n",
    "    for i in data:\n",
    "        d = dist_euc(avg, i[:-1])\n",
    "        if d < dist[0]:\n",
    "            dist[0] = d\n",
    "            keys[0] = i[-1]\n",
    "\n",
    "        d = dist_man(avg, i[:-1])\n",
    "        if d < dist[1]:\n",
    "            dist[1] = d\n",
    "            keys[1] = i[-1]\n",
    "\n",
    "        d = dist_cos(avg, i[:-1])\n",
    "        if d > dist[2]:\n",
    "            dist[2] = d\n",
    "            keys[2] = i[-1]\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "avg = {'Iris-setosa': 0, 'Iris-versicolor': 0, 'Iris-virginica': 0}\n",
    "P6Iris(train, test, key_iris, avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7\\\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\\\n",
    "    1) Divide your data into random train and test sets\\\n",
    "    2) Training: k-means clustering for k=1 to k=15 (i.e., run your algorithm 15 times). DO NOT USE LABLES. Give the centroids. In case of mint, give this as k images, one for each label. (you may use a library function)\\\n",
    "    3) Label a cluster using the majority label found in the cluster. \\\n",
    "    4) For each data point in the test set, find the closest from the centroid Use Manhattan, Euclidean and Cosine distances (note that higher cosine value means lower distance). Give the point that label. \\\n",
    "    5) Give the confusion matrix for training and test sets.\\\n",
    "    6) Form the elbow map - using %correct as a measure. (Test set accuracy)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4\n",
      "Right: 102\t Wrong: 17\n",
      "[[40, 0, 0], [0, 22, 16], [0, 1, 40]]\n",
      "Right: 27\t Wrong: 3\n",
      "[[9, 0, 0], [0, 9, 3], [0, 0, 9]]\n",
      "Right: 101\t Wrong: 18\n",
      "[[40, 0, 0], [0, 21, 17], [0, 1, 40]]\n",
      "Right: 26\t Wrong: 4\n",
      "[[9, 0, 0], [0, 8, 4], [0, 0, 9]]\n",
      "Right: 111\t Wrong: 8\n",
      "[[40, 0, 0], [0, 30, 8], [0, 0, 41]]\n",
      "Right: 29\t Wrong: 1\n",
      "[[9, 0, 0], [0, 11, 1], [0, 0, 9]]\n"
     ]
    }
   ],
   "source": [
    "# Finds and returns the index containing the max element of a list\n",
    "def max_index(lst):\n",
    "    max = 0\n",
    "    index = 0\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] > max:\n",
    "            max = lst[i]\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "\"\"\"\n",
    "Finds the cluster a given data point belongs to for each distance method\n",
    "clusters: a list of the centroid of each cluster\n",
    "data: the vector in need of classification\n",
    "Return: the labels generated by each distance calculation method\n",
    "\"\"\"\n",
    "def find_cluster(clusters, data):\n",
    "    dist = [m.inf, m.inf, 0]    # Define maximum distances\n",
    "    labels = [0, 0, 0]          # Initalize labels\n",
    "    # Iterate through clusters to find best fit\n",
    "    for i in range(len(clusters)):\n",
    "        d = dist_euc(clusters[i], data[:-1])\n",
    "        if d < dist[0]:\n",
    "            dist[0] = d\n",
    "            labels[0] = i\n",
    "\n",
    "        d = dist_man(clusters[i], data[:-1])\n",
    "        if d < dist[1]:\n",
    "            dist[1] = d\n",
    "            labels[1] = i\n",
    "\n",
    "        d = dist_cos(clusters[i], data[:-1])\n",
    "        if d > dist[2]:\n",
    "            dist[2] = d\n",
    "            labels[2] = i\n",
    "\n",
    "    return labels\n",
    "\n",
    "\"\"\"\n",
    "Generalized algorithm for problem 7 using the iris dataset\n",
    "train: training data set\n",
    "test: testing data set\n",
    "key: dictionary used to convert to and from string labels\n",
    "kmin: minimum number of clusters\n",
    "kmax: maximum number of clusters\n",
    "\"\"\"\n",
    "def P7Iris(train, test, key, kmin, kmax):\n",
    "    train_lab = [i[:-1] for i in train] # Remove labels from training data\n",
    "    # Run k means clustering from k = kmin to k = kmax\n",
    "    for k in range(kmin, kmax + 1):\n",
    "        # Tracks the label and number of elements for each cluster\n",
    "        cluster_count = {}\n",
    "        kmeans = skc.KMeans(k).fit(train_lab)\n",
    "        labels = kmeans.labels_\n",
    "        # Counts the actual labels of each cluster\n",
    "        for i in range(len(train)):\n",
    "            if labels[i] not in cluster_count:\n",
    "                cluster_count[labels[i]] = [0, 0, 0]\n",
    "            cluster_count[labels[i]][key[train[i][-1]]] += 1\n",
    "        # Assigns a label to each cluster\n",
    "        for i in cluster_count.keys():\n",
    "            cluster_count[i] = (cluster_count[i], key[max_index(cluster_count[i])])\n",
    "        # print(cluster_count) # Sanity check\n",
    "    \n",
    "        # Get cluster centroids\n",
    "        clusters = kmeans.cluster_centers_\n",
    "\n",
    "        # Accuracy measures for each distance method for training and test sets\n",
    "        # [# correct, # Incorrect]\n",
    "        acc_train = [[0, 0] for i in range(3)]\n",
    "        acc_test = [[0, 0] for i in range(3)]\n",
    "\n",
    "        \n",
    "        # Initialize confusion Matrices\n",
    "        conf_train = [[[0, 0, 0], [0, 0, 0], [0, 0, 0]] for i in range(3)]\n",
    "        conf_test  = [[[0, 0, 0], [0, 0, 0], [0, 0, 0]] for i in range(3)]\n",
    "\n",
    "        # Find the number correct and incorrect for each distance method in training set\n",
    "        for i in range(len(train)):\n",
    "            # Find a cluster for an individual element\n",
    "            lab = find_cluster(clusters, train[i])\n",
    "            # For each distance method, establish if each was correct or not\n",
    "            for j in range(len(lab)):\n",
    "                if train[i][-1] == cluster_count[lab[j]][1]:\n",
    "                    acc_train[j][0] += 1\n",
    "                else:\n",
    "                    acc_train[j][1] += 1\n",
    "                # Add an entry to the confusion matrix\n",
    "                conf_train[j][key[train[i][-1]]][key[cluster_count[lab[j]][1]]] += 1\n",
    "\n",
    "        # Find the number correct and incorrect for each distance method in test set\n",
    "        for i in range(len(test)):\n",
    "            # Find a cluster for an individual element\n",
    "            lab = find_cluster(clusters, test[i])\n",
    "            # For each distance method, establish if each was correct or not\n",
    "            for j in range(len(lab)):\n",
    "                if test[i][-1] == cluster_count[lab[j]][1]:\n",
    "                    acc_test[j][0] += 1\n",
    "                else:\n",
    "                    acc_test[j][1] += 1\n",
    "                # Add an entry to the confusion matrix\n",
    "                conf_test[j][key[test[i][-1]]][key[cluster_count[lab[j]][1]]] += 1\n",
    "\n",
    "        # Print off results in the form of accuracy percentages\n",
    "        print(f\"k = {k}\")\n",
    "        for i in range(3):\n",
    "            print(f\"Right: {acc_train[i][0]}\\t Wrong: {acc_train[i][1]}\")\n",
    "            print(conf_train[i])\n",
    "            print(f\"Right: {acc_test[i][0]}\\t Wrong: {acc_test[i][1]}\")\n",
    "            print(conf_test[i])\n",
    "\n",
    "P7Iris(train, test, key_iris, 4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8\\\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\\\n",
    "    1) Divide your data into random train and test sets\\\n",
    "    2) Using kNN with Manhattan, Euclidean and cosine distances classify the training and test sets. Do this for k=1, k=5, k=10. (you may use a library function)\\\n",
    "    3) Give the confusion matrix for training and test sets.\\\n",
    "    4) Form the elbow map - using %correct as a measure. (Test set accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e9548d0f655117a047e388695746c0ee220d22be9f30de4f4d8bfc50335b506"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
