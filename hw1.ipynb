{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import sklearn.cluster as skc\n",
    "import sklearn.neighbors as skn\n",
    "from PIL import Image as img\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into a training and testing set\n",
    "D: dataset given as input\n",
    "percent: the percentage of data used for testing\n",
    "Return: a tupple containing both the training and testing data\n",
    "\"\"\"\n",
    "def split(D, percent):\n",
    "    used = []   # Track indeces used for test set\n",
    "    test = []   # List of vectors for test set\n",
    "    train = []  # List of vectors for training set\n",
    "\n",
    "    # Add random elements into the training set until a threshold\n",
    "    while len(test) < len(D) * percent:\n",
    "        i = rd.randint(0, len(D) - 1)\n",
    "        if i not in used:\n",
    "            used.append(i)\n",
    "            test.append(D.iloc[i].values.tolist())\n",
    "\n",
    "    # Add all remaining elements into the training set\n",
    "    for i in range(len(D)):\n",
    "        if i not in used:\n",
    "            train.append(D.iloc[i].values.tolist())\n",
    "    return (train, test)\n",
    "\n",
    "\"\"\"\n",
    "Splits a given data set into features and labels.\n",
    "IMPORTANT: Assumes data label is the last column of each vector\n",
    "data: The dataset being split\n",
    "Returns a list of features and a list of labels associated with each vector\n",
    "\"\"\"\n",
    "def split_labels(data, last=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in data:\n",
    "        if (last):\n",
    "            features.append(i[:-1])\n",
    "            labels.append(i[-1])\n",
    "        else:\n",
    "            features.append(i[1:])\n",
    "            labels.append(i[0])\n",
    "    return features, labels\n",
    "\n",
    "\"\"\"\n",
    "Prints a confusion matrix with one label per line\n",
    "conf: the confusion matrix input\n",
    "key: key dictionary to translate from integer to string labels\n",
    "\"\"\"\n",
    "def print_conf(conf, key):\n",
    "    # Print Label header\n",
    "    for i in range(len(conf)):\n",
    "        prnt = \"\"\n",
    "        for j in range(len(conf[i])):\n",
    "            prnt += str(conf[i][j]) + \", \"\n",
    "        print(prnt[:-2])\n",
    "\n",
    "\"\"\"\n",
    "Calculates Eclidian distance of two lists of floats\n",
    "p1: first vector\n",
    "p2: second vector\n",
    "Return: calculated Euclidian distance\n",
    "\"\"\"\n",
    "def dist_euc(p1, p2):\n",
    "    dist = 0\n",
    "    # Compares the length of p1 and p2 and returns an error if uneven\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        # Iterate over each featur finding (f_p1 - f_p2)^2\n",
    "        for i in range(len(p1)):\n",
    "            dist += (p1[i] - p2[i]) ** 2\n",
    "        # Take the square root of the sum and return\n",
    "        return m.sqrt(dist)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculates Manhattan distance of two lists of floats\n",
    "p1: first vector\n",
    "p2: second vector\n",
    "Return: calculated Manhattan distance\n",
    "\"\"\"\n",
    "def dist_man(p1, p2):\n",
    "    dist = 0\n",
    "    # Compares the length of p1 and p2 and returns an error if uneven\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        for i in range(len(p1)):\n",
    "            dist += abs(p1[i] - p2[i])\n",
    "        return dist\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculates Cosine similarity of two lists of floats\n",
    "p1: first vector\n",
    "p2: second vector\n",
    "Return: calculated Cosine similarity\n",
    "\"\"\"\n",
    "def dist_cos(p1, p2):\n",
    "    dist = 0\n",
    "    # Compares the length of p1 and p2 and returns an error if uneven\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        l = len(p1)\n",
    "        # Takes the dot product of the two vectors\n",
    "        dot = sum(i[0] * i[1] for i in zip(p1, p2))\n",
    "        return dot / (dist_euc([0] * l, p1) * dist_euc([0] * l, p2))\n",
    "\n",
    "def print_mnist(data, row=5, column=2, h=28, w=28):\n",
    "    plt.figure(figsize=(h, w))\n",
    "    for i in range(len(data)):\n",
    "        plt.subplot(column, row, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(data[i], cmap=plt.cm.binary)\n",
    "    plt.show()\n",
    "\n",
    "def format_array(data, h=28, w=28):\n",
    "    ret = []\n",
    "    for i in range(len(data)):\n",
    "        row = []\n",
    "        count = 0\n",
    "        for j in range(h):\n",
    "            col = []\n",
    "            for k in range(w):\n",
    "                col.append(data[i][count])\n",
    "                count += 1\n",
    "            row.append(col)\n",
    "        ret.append(row)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split(pd.read_csv('Datasets/iris.csv'), 0.2)\n",
    "\n",
    "mtrain = np.array(pd.read_csv('mnist_train.csv', header=None).values.tolist())\n",
    "mtest = np.array(pd.read_csv('mnist_test.csv', header=None).values.tolist())\n",
    "\n",
    "key_iris = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2,\n",
    "            0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'}\n",
    "\n",
    "key_mnist = {0: \"Zero\", 1: \"One\", 2: \"Two\", 3: \"Three\", 4: \"Four\", 5: \"Five\", 6: \"Six\", 7: \"Seven\", 8: \"Eight\", 9: \"Nine\",\n",
    "             \"Zero\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5, \"Six\": 6, \"Seven\": 7, \"Eight\": 8, \"Nine\": 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=[]\n",
    "\n",
    "\n",
    "print_mnist(tester)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6\\\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\\\n",
    "    1) Divide your data into random train and test sets\\\n",
    "    2) Training: for a specific label, find the average of each feature. (In case of mnist, this is pixel wise).  give your averages. In case of mint, give this as 10 images, one for each label\\\n",
    "    3) For each data point in the training set, find the closest from the average templates. Use Manhattan, Euclidean and Cosine distances (note that higher cosine value means lower distance). Give the point that label. \\\n",
    "    4) Do the same for the test set\\\n",
    "    5) Give the confusion matrix for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs the algoritm needed to complete problem 6 for the iris dataset\n",
    "train: the training set used in the algorithm\n",
    "test: the testing set used to validate results\n",
    "key: a dictionary to convert from text labels to integer representation and back\n",
    "avg: a dictonary used to keep track of average values for each label in the data\n",
    "\"\"\"\n",
    "def P6Iris(train, test, key, avg):\n",
    "    # Separate the data by label and take the average of each\n",
    "    for row in train:\n",
    "        # Initialize each label with the first occurrance\n",
    "        if avg[row[4]] == 0:\n",
    "            avg[row[4]] = [row[0:4], 1]\n",
    "        else:\n",
    "            avg[row[4]][1] += 1\n",
    "            # Take a rolling average for each feature\n",
    "            for i in range(len(row) - 1):\n",
    "                avg[row[4]][0][i] = avg[row[4]][0][i] + (row[i] - avg[row[4]][0][i]) / avg[row[4]][1]\n",
    "\n",
    "    # Define lists of closest values\n",
    "    train_vals = []\n",
    "    test_vals = []\n",
    "    # Find the closest points to each label using the 3 measurement methods\n",
    "    for val in avg.values():\n",
    "        train_vals.append(find_min_dist(val[0], train))\n",
    "        test_vals.append(find_min_dist(val[0], test))\n",
    "\n",
    "    # Define confusion matrices\n",
    "    conf_train = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "    conf_test  = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "\n",
    "    # Populate confusion matrices\n",
    "    for i in range(len(avg)):\n",
    "        for j in range(len(avg)):\n",
    "            conf_train[i][key[train_vals[i][j]]] += 1\n",
    "            conf_test[i][key[test_vals[i][j]]] += 1\n",
    "\n",
    "    # Print data relavant to the problem\n",
    "    print(avg)\n",
    "    print(train_vals)\n",
    "    print(conf_train)\n",
    "    print(test_vals)\n",
    "    print(conf_test)\n",
    "\n",
    "\"\"\"\n",
    "Finds the minimum distance between an average point and the dataset using\n",
    "    Euclidian distance, Manhattan distance, and Cosine similarity\n",
    "avg: the average values for a given label\n",
    "data: the raw dataset being tested against\n",
    "Return: a set of points representing minumum distance for each distance measure\n",
    "\"\"\"\n",
    "def find_min_dist(avg, data):\n",
    "    # Define maximum possible distances (0 in case of cosine similarity)\n",
    "    dist = [m.inf, m.inf, 0]\n",
    "    keys = [[], [], []]\n",
    "    # Iterate through the dataset and find min distances\n",
    "    for i in data:\n",
    "        d = dist_euc(avg, i[:-1])\n",
    "        if d < dist[0]:\n",
    "            dist[0] = d\n",
    "            keys[0] = i[-1]\n",
    "\n",
    "        d = dist_man(avg, i[:-1])\n",
    "        if d < dist[1]:\n",
    "            dist[1] = d\n",
    "            keys[1] = i[-1]\n",
    "\n",
    "        d = dist_cos(avg, i[:-1])\n",
    "        if d > dist[2]:\n",
    "            dist[2] = d\n",
    "            keys[2] = i[-1]\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "avg = {'Iris-setosa': 0, 'Iris-versicolor': 0, 'Iris-virginica': 0}\n",
    "P6Iris(train, test, key_iris, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea\n",
    "#Loop once, counting # of each digit\n",
    "#Make arrays for avg's\n",
    "#Loop through again, this time storing avg's in the arrays made prior\n",
    "#Loop one more time. For each thing in the training set, see which avg image it's closest to. Assign it that label\n",
    "#Repeat above step for test set\n",
    "#Generate & print confusion\n",
    "\n",
    "def getActualLabel(digit):\n",
    "    return digit[0]\n",
    "\n",
    "def labelFromEuclidean(digit, avgList):\n",
    "    dist = m.inf\n",
    "    label = 0\n",
    "    \n",
    "    #Go through the average list, find which one we're closest to\n",
    "    for i in range(10):\n",
    "        testDigit = avgList[i]\n",
    "        currentDist = dist_euc(digit[1:], testDigit)\n",
    "        if currentDist < dist:\n",
    "            dist = currentDist\n",
    "            label = i\n",
    "            \n",
    "    return label\n",
    "\n",
    "def labelFromManhattan(digit, avgList):\n",
    "    dist = m.inf\n",
    "    label = 0\n",
    "    \n",
    "    #Go through the average list, find which one we're closest to\n",
    "    for i in range(10):\n",
    "        testDigit = avgList[i]\n",
    "        currentDist = dist_man(digit[1:], testDigit)\n",
    "        if currentDist < dist:\n",
    "            dist = currentDist\n",
    "            label = i\n",
    "            \n",
    "    return label\n",
    "\n",
    "def labelFromCosine(digit, avgList):\n",
    "    dist = 0\n",
    "    label = 0\n",
    "    \n",
    "    #Go through the average list, find which one we're closest to\n",
    "    for i in range(10):\n",
    "        testDigit = avgList[i]\n",
    "        currentDist = dist_cos(digit[1:], testDigit)\n",
    "        if currentDist > dist:\n",
    "            dist = currentDist\n",
    "            label = i\n",
    "            \n",
    "    return label\n",
    "\n",
    "def p6MNist():\n",
    "    #Make a list to count the labels\n",
    "    labelCounts = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    #Read the csv files, make some lists\n",
    "    mTrain = pd.read_csv(\"mnist_train.csv\", header = None).values.tolist()\n",
    "    mTest = pd.read_csv(\"mnist_test.csv\", header = None).values.tolist()\n",
    "    \n",
    "    #Count the number of each label\n",
    "    for i in mTrain:\n",
    "        labelCounts[i[0]] += 1\n",
    "        \n",
    "    print(labelCounts)\n",
    "    \n",
    "    #Make a list to store the avg digits\n",
    "    avgLists = []\n",
    "    \n",
    "    #Do an iteration for each digit label\n",
    "    for i in range(10):\n",
    "        avgDigit = [0 for i in range(28 ** 2)]\n",
    "        \n",
    "        #Loop through the list of digits\n",
    "        for j in mTrain:\n",
    "            #Skip if it's not the current digit label\n",
    "            if j[0] != i:\n",
    "                continue\n",
    "                \n",
    "            #Otherwise, compound this digit onto the rolling average\n",
    "            for k in range(28 ** 2):\n",
    "                actualIndex = k + 1\n",
    "                avgDigit[k] += j[actualIndex] / labelCounts[i]\n",
    "                \n",
    "        avgLists.append(avgDigit)\n",
    "        \n",
    "    print(len(avgLists))\n",
    "    print_mnist(format_array(avgLists))\n",
    "        \n",
    "    #Assign labels to training set based on proximity to avg image\n",
    "    trainLabelsEuc = []\n",
    "    trainLabelsMan = []\n",
    "    trainLabelsCos = []\n",
    "    \n",
    "    #Remove [0:9] for full set\n",
    "    for i in mTrain:\n",
    "        trainLabelsEuc.append(labelFromEuclidean(i, avgLists))\n",
    "        trainLabelsMan.append(labelFromManhattan(i, avgLists))\n",
    "        trainLabelsCos.append(labelFromCosine(i, avgLists))\n",
    "        \n",
    "    #Do the same thing for the test set\n",
    "    testLabelsEuc = []\n",
    "    testLabelsMan = []\n",
    "    testLabelsCos = []\n",
    "    \n",
    "    #Remove [0:9] for full set\n",
    "    for i in mTest:\n",
    "        testLabelsEuc.append(labelFromEuclidean(i, avgLists))\n",
    "        testLabelsMan.append(labelFromManhattan(i, avgLists))\n",
    "        testLabelsCos.append(labelFromCosine(i, avgLists))\n",
    "        \n",
    "    #Make the confusion matrices\n",
    "    #For the training euclidean\n",
    "    trainEConfusion = [[0 for i in range(10)] for i in range(10)]\n",
    "    for i in range(len(trainLabelsEuc)):\n",
    "        trainEConfusion[getActualLabel(mTrain[i])][trainLabelsEuc[i]] += 1\n",
    "    #Print the matrix\n",
    "    print(\"\\nTraining Euclidean Matrix\")\n",
    "    for i in trainEConfusion:\n",
    "        print(i)\n",
    "        \n",
    "    #For the training manhatten\n",
    "    trainMConfusion = [[0 for i in range(10)] for i in range(10)]\n",
    "    for i in range(len(trainLabelsMan)):\n",
    "        trainMConfusion[getActualLabel(mTrain[i])][trainLabelsMan[i]] += 1\n",
    "    #Print the matrix\n",
    "    print(\"\\nTraining Manhattan Matrix\")\n",
    "    for i in trainMConfusion:\n",
    "        print(i)\n",
    "        \n",
    "    #For the training cosine\n",
    "    trainCConfusion = [[0 for i in range(10)] for i in range(10)]\n",
    "    for i in range(len(trainLabelsCos)):\n",
    "        trainCConfusion[getActualLabel(mTrain[i])][trainLabelsCos[i]] += 1\n",
    "    #Print the matrix\n",
    "    print(\"\\nTraining Cosine Matrix\")\n",
    "    for i in trainCConfusion:\n",
    "        print(i)\n",
    "        \n",
    "    #For the test euclidean\n",
    "    testEConfusion = [[0 for i in range(10)] for i in range(10)]\n",
    "    for i in range(len(testLabelsEuc)):\n",
    "        testEConfusion[getActualLabel(mTest[i])][testLabelsEuc[i]] += 1\n",
    "    #Print the matrix\n",
    "    print(\"\\nTesting Euclidean Matrix\")\n",
    "    for i in testEConfusion:\n",
    "        print(i)\n",
    "        \n",
    "    #For the testing manhatten\n",
    "    testMConfusion = [[0 for i in range(10)] for i in range(10)]\n",
    "    for i in range(len(testLabelsMan)):\n",
    "        testMConfusion[getActualLabel(mTest[i])][testLabelsMan[i]] += 1\n",
    "    #Print the matrix\n",
    "    print(\"\\nTesting Manhattan Matrix\")\n",
    "    for i in testMConfusion:\n",
    "        print(i)\n",
    "        \n",
    "    #For the testing cosine\n",
    "    testCConfusion = [[0 for i in range(10)] for i in range(10)]\n",
    "    for i in range(len(testLabelsCos)):\n",
    "        testCConfusion[getActualLabel(mTest[i])][testLabelsCos[i]] += 1\n",
    "    #Print the matrix\n",
    "    print(\"\\nTesting Cosine Matrix\")\n",
    "    for i in testCConfusion:\n",
    "        print(i)\n",
    "    \n",
    "p6MNist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7\\\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\\\n",
    "    1) Divide your data into random train and test sets\\\n",
    "    2) Training: k-means clustering for k=1 to k=15 (i.e., run your algorithm 15 times). DO NOT USE LABLES. Give the centroids. In case of mint, give this as k images, one for each label. (you may use a library function)\\\n",
    "    3) Label a cluster using the majority label found in the cluster. \\\n",
    "    4) For each data point in the test set, find the closest from the centroid Use Manhattan, Euclidean and Cosine distances (note that higher cosine value means lower distance). Give the point that label. \\\n",
    "    5) Give the confusion matrix for training and test sets.\\\n",
    "    6) Form the elbow map - using %correct as a measure. (Test set accuracy)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds and returns the index containing the max element of a list\n",
    "def max_index(lst):\n",
    "    max = 0\n",
    "    index = 0\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] > max:\n",
    "            max = lst[i]\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "\"\"\"\n",
    "Finds the cluster a given data point belongs to for each distance method\n",
    "clusters: a list of the centroid of each cluster\n",
    "data: the vector in need of classification\n",
    "Return: the labels generated by each distance calculation method\n",
    "\"\"\"\n",
    "def find_cluster(clusters, data):\n",
    "    dist = [m.inf, m.inf, 0]    # Define maximum distances\n",
    "    labels = [0, 0, 0]          # Initalize labels\n",
    "    # Iterate through clusters to find best fit\n",
    "    for i in range(len(clusters)):\n",
    "        d = dist_euc(clusters[i], data[:-1])\n",
    "        if d < dist[0]:\n",
    "            dist[0] = d\n",
    "            labels[0] = i\n",
    "\n",
    "        d = dist_man(clusters[i], data[:-1])\n",
    "        if d < dist[1]:\n",
    "            dist[1] = d\n",
    "            labels[1] = i\n",
    "\n",
    "        d = dist_cos(clusters[i], data[:-1])\n",
    "        if d > dist[2]:\n",
    "            dist[2] = d\n",
    "            labels[2] = i\n",
    "\n",
    "    return labels\n",
    "\n",
    "\"\"\"\n",
    "Generalized algorithm for problem 7 using the iris dataset\n",
    "train: training data set\n",
    "test: testing data set\n",
    "key: dictionary used to convert to and from string labels\n",
    "kmin: minimum number of clusters\n",
    "kmax: maximum number of clusters\n",
    "\"\"\"\n",
    "def P7Iris(train, test, key, kmin, kmax):\n",
    "    train_lab = [i[:-1] for i in train] # Remove labels from training data\n",
    "    # Run k means clustering from k = kmin to k = kmax\n",
    "    for k in range(kmin, kmax + 1):\n",
    "        # Tracks the label and number of elements for each cluster\n",
    "        cluster_count = {}\n",
    "        kmeans = skc.KMeans(k).fit(train_lab)\n",
    "        labels = kmeans.labels_\n",
    "        # Counts the actual labels of each cluster\n",
    "        for i in range(len(train)):\n",
    "            if labels[i] not in cluster_count:\n",
    "                cluster_count[labels[i]] = [0, 0, 0]\n",
    "            cluster_count[labels[i]][key[train[i][-1]]] += 1\n",
    "        # Assigns a label to each cluster\n",
    "        for i in cluster_count.keys():\n",
    "            cluster_count[i] = (cluster_count[i], key[max_index(cluster_count[i])])\n",
    "        # print(cluster_count) # Sanity check\n",
    "    \n",
    "        # Get cluster centroids\n",
    "        clusters = kmeans.cluster_centers_\n",
    "\n",
    "        # Accuracy measures for each distance method for training and test sets\n",
    "        # [# correct, # Incorrect]\n",
    "        acc_train = [[0, 0] for i in range(3)]\n",
    "        acc_test = [[0, 0] for i in range(3)]\n",
    "\n",
    "        \n",
    "        # Initialize confusion Matrices\n",
    "        conf_train = [[[0, 0, 0], [0, 0, 0], [0, 0, 0]] for i in range(3)]\n",
    "        conf_test  = [[[0, 0, 0], [0, 0, 0], [0, 0, 0]] for i in range(3)]\n",
    "\n",
    "        # Find the number correct and incorrect for each distance method in training set\n",
    "        for i in range(len(train)):\n",
    "            # Find a cluster for an individual element\n",
    "            lab = find_cluster(clusters, train[i])\n",
    "            # For each distance method, establish if each was correct or not\n",
    "            for j in range(len(lab)):\n",
    "                if train[i][-1] == cluster_count[lab[j]][1]:\n",
    "                    acc_train[j][0] += 1\n",
    "                else:\n",
    "                    acc_train[j][1] += 1\n",
    "                # Add an entry to the confusion matrix\n",
    "                conf_train[j][key[train[i][-1]]][key[cluster_count[lab[j]][1]]] += 1\n",
    "\n",
    "        # Find the number correct and incorrect for each distance method in test set\n",
    "        for i in range(len(test)):\n",
    "            # Find a cluster for an individual element\n",
    "            lab = find_cluster(clusters, test[i])\n",
    "            # For each distance method, establish if each was correct or not\n",
    "            for j in range(len(lab)):\n",
    "                if test[i][-1] == cluster_count[lab[j]][1]:\n",
    "                    acc_test[j][0] += 1\n",
    "                else:\n",
    "                    acc_test[j][1] += 1\n",
    "                # Add an entry to the confusion matrix\n",
    "                conf_test[j][key[test[i][-1]]][key[cluster_count[lab[j]][1]]] += 1\n",
    "\n",
    "        # Print off results in the form of accuracy percentages\n",
    "        print(f\"k = {k}\")\n",
    "        for i in range(3):\n",
    "            print(f\"Right: {acc_train[i][0]}\\t Wrong: {acc_train[i][1]}\")\n",
    "            print_conf(conf_train[i], key)\n",
    "            print(f\"Right: {acc_test[i][0]}\\t Wrong: {acc_test[i][1]}\")\n",
    "            print_conf(conf_test[i], key)\n",
    "\n",
    "P7Iris(train, test, key_iris, 4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds and returns the index containing the max element of a list\n",
    "def max_index(lst):\n",
    "    max = 0\n",
    "    index = 0\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] > max:\n",
    "            max = lst[i]\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Finds the cluster a given data point belongs to for each distance method\n",
    "clusters: a list of the centroid of each cluster\n",
    "data: the vector in need of classification\n",
    "Return: the labels generated by each distance calculation method\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def find_cluster(clusters, data):\n",
    "    dist = [m.inf, m.inf, 0]    # Define maximum distances\n",
    "    labels = [0, 0, 0]          # Initalize labels\n",
    "    # Iterate through clusters to find best fit\n",
    "    for i in range(len(clusters)):\n",
    "        d = dist_euc(clusters[i], data[:-1])\n",
    "        if d < dist[0]:\n",
    "            dist[0] = d\n",
    "            labels[0] = i\n",
    "\n",
    "        d = dist_man(clusters[i], data[:-1])\n",
    "        if d < dist[1]:\n",
    "            dist[1] = d\n",
    "            labels[1] = i\n",
    "\n",
    "        d = dist_cos(clusters[i], data[:-1])\n",
    "        if d > dist[2]:\n",
    "            dist[2] = d\n",
    "            labels[2] = i\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generalized algorithm for problem 7 using the iris dataset\n",
    "train: training data set\n",
    "test: testing data set\n",
    "key: dictionary used to convert to and from string labels\n",
    "kmin: minimum number of clusters\n",
    "kmax: maximum number of clusters\n",
    "\"\"\"\n",
    "def P7MNIST(train, test, key, kmin, kmax):\n",
    "    measure = {0: \"Euc\", 1: \"Man\", 2: \"Cos\"}\n",
    "    train_lab = [i[1:] for i in train]  # Remove labels from training data\n",
    "    # Run k means clustering from k = kmin to k = kmax\n",
    "    for k in range(kmin, kmax + 1):\n",
    "        # Tracks the label and number of elements for each cluster\n",
    "        cluster_count = {}\n",
    "        kmeans = skc.KMeans(k).fit(train_lab)\n",
    "        labels = kmeans.labels_\n",
    "        # Counts the actual labels of each cluster\n",
    "        for i in range(len(train)):\n",
    "            if labels[i] not in cluster_count:\n",
    "                cluster_count[labels[i]] = [0 for i in range(10)]\n",
    "            cluster_count[labels[i]][train[i][0]] += 1\n",
    "        # Assigns a label to each cluster\n",
    "        for i in cluster_count.keys():\n",
    "            cluster_count[i] = (cluster_count[i], max_index(cluster_count[i]))\n",
    "        # print(cluster_count) # Sanity check\n",
    "\n",
    "        # Get cluster centroids\n",
    "        clusters = kmeans.cluster_centers_\n",
    "        # print_mnist(format_array(clusters), column=3)\n",
    "\n",
    "        # Accuracy measures for each distance method for training and test sets\n",
    "        # [# correct, # Incorrect]\n",
    "        acc_train = [[0, 0] for i in range(3)]\n",
    "        acc_test = [[0, 0] for i in range(3)]\n",
    "\n",
    "        # Initialize confusion Matrices\n",
    "        conf_train = [[[0 for i in range(10)]\n",
    "                       for i in range(10)] for i in range(3)]\n",
    "        conf_test = [[[0 for i in range(10)]\n",
    "                      for i in range(10)] for i in range(3)]\n",
    "\n",
    "        # Find the number correct and incorrect for each distance method in training set\n",
    "        for i in range(len(train)):\n",
    "            # Find a cluster for an individual element\n",
    "            lab = find_cluster(clusters, train[i])\n",
    "            # For each distance method, establish if each was correct or not\n",
    "            for j in range(len(lab)):\n",
    "                if train[i][0] == cluster_count[lab[j]][1]:\n",
    "                    acc_train[j][0] += 1\n",
    "                else:\n",
    "                    acc_train[j][1] += 1\n",
    "                # Add an entry to the confusion matrix\n",
    "                conf_train[j][train[i][0]][cluster_count[lab[j]][1]] += 1\n",
    "\n",
    "        # Find the number correct and incorrect for each distance method in test set\n",
    "        for i in range(len(test)):\n",
    "            # Find a cluster for an individual element\n",
    "            lab = find_cluster(clusters, test[i])\n",
    "            # For each distance method, establish if each was correct or not\n",
    "            for j in range(len(lab)):\n",
    "                if test[i][0] == cluster_count[lab[j]][1]:\n",
    "                    acc_test[j][0] += 1\n",
    "                else:\n",
    "                    acc_test[j][1] += 1\n",
    "                # Add an entry to the confusion matrix\n",
    "                conf_test[j][test[i][0]][cluster_count[lab[j]][1]] += 1\n",
    "\n",
    "        # Print off results in the form of accuracy percentages\n",
    "        print(f\"k = {k}\")\n",
    "        for i in range(3):\n",
    "            # print(f\"Right: {acc_train[i][0]}\\t Wrong: {acc_train[i][1]}\")\n",
    "            print_conf(conf_train[i], key)\n",
    "            # print(f\"Right: {acc_test[i][0]}\\t Wrong: {acc_test[i][1]}\")\n",
    "            print_conf(conf_test[i], key)\n",
    "            # print(\"{} accuracy for k={}: {:.1f}\".format(measure[i],\n",
    "                # k, (acc_test[i][0] / (acc_test[i][1] + acc_test[i][0])) * 100))\n",
    "\n",
    "\n",
    "P7MNIST(mtrain[:5000], mtest[:1000], key_mnist, 12, 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8\\\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\\\n",
    "    1) Divide your data into random train and test sets\\\n",
    "    2) Using kNN with Manhattan, Euclidean and cosine distances classify the training and test sets. Do this for k=1, k=5, k=10. (you may use a library function)\\\n",
    "    3) Give the confusion matrix for training and test sets.\\\n",
    "    4) Form the elbow map - using %correct as a measure. (Test set accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Algorithm used to complete problem 8 of HW1 with the iris.tab dataset\n",
    "train: matrix of training data vectors\n",
    "test: matrix of testing data vectors\n",
    "key: Key necessary to translate between string and int label representation\n",
    "k: number of neighbors for the KNN classifier\n",
    "\"\"\"\n",
    "def P8Iris(train, test, key, k):\n",
    "    # Split the training and testing sets into features and labels\n",
    "    train_features, train_labels = split_labels(train)\n",
    "    test_features, test_labels = split_labels(test)\n",
    "    \n",
    "    # Initialize KNN Classifiers for each distance metric\n",
    "    knn = []\n",
    "    knn.append(skn.KNeighborsClassifier(n_neighbors=k, metric=\"euclidean\"))\n",
    "    knn.append(skn.KNeighborsClassifier(n_neighbors=k, metric=\"cityblock\"))\n",
    "    knn.append(skn.KNeighborsClassifier(n_neighbors=k, metric=\"cosine\"))\n",
    "\n",
    "    # Initialize confusion matrices for training and testing across metrics\n",
    "    conf_train = [[[0, 0, 0], [0, 0, 0], [0, 0, 0]] for i in range(3)]\n",
    "    conf_test = [[[0, 0, 0], [0, 0, 0], [0, 0, 0]] for i in range(3)]\n",
    "\n",
    "    # For each metric, generate confustion matrix and print accuracy\n",
    "    for i in range(3):\n",
    "        # Train the model on training data\n",
    "        knn[i].fit(train_features, train_labels)\n",
    "\n",
    "        # Generate prediction data for the training set\n",
    "        pred_labels = knn[i].predict(train_features)\n",
    "        # Build confusion matrix for training set\n",
    "        for j in range(len(train_labels)):\n",
    "            conf_train[i][key[train_labels[j]]][key[pred_labels[j]]] += 1\n",
    "\n",
    "        # Generate prediction data for the test set\n",
    "        pred_labels = knn[i].predict(test_features)\n",
    "        # Build confusion matrix for test set\n",
    "        for j in range(len(test_labels)):\n",
    "            conf_test[i][key[test_labels[j]]][key[pred_labels[j]]] += 1\n",
    "\n",
    "        # Print out the training and testing confusion matrices\n",
    "        print_conf(conf_train[i], key)\n",
    "        print()\n",
    "        print_conf(conf_test[i], key)\n",
    "\n",
    "        # Print out % accuracy data\n",
    "        print(\"{:.1f}\\n\".format(knn[i].score(test_features, test_labels) * 100))\n",
    "\n",
    "\n",
    "P8Iris(train, test, key_iris, 1)\n",
    "P8Iris(train, test, key_iris, 5)\n",
    "P8Iris(train, test, key_iris, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P8Iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26300/338630881.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mP8Iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_iris\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0mP8Iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_iris\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mP8Iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_iris\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'P8Iris' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Algorithm used to complete problem 8 of HW1 with the iris.tab dataset\n",
    "train: matrix of training data vectors\n",
    "test: matrix of testing data vectors\n",
    "key: Key necessary to translate between string and int label representation\n",
    "k: number of neighbors for the KNN classifier\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def P8MNIST(train, test, key, k):\n",
    "    # Split the training and testing sets into features and labels\n",
    "    train_features, train_labels = split_labels(train, last=False)\n",
    "    test_features, test_labels = split_labels(test, last=False)\n",
    "\n",
    "    # Initialize KNN Classifiers for each distance metric\n",
    "    knn = []\n",
    "    knn.append(skn.KNeighborsClassifier(n_neighbors=k, metric=\"euclidean\"))\n",
    "    knn.append(skn.KNeighborsClassifier(n_neighbors=k, metric=\"cityblock\"))\n",
    "    knn.append(skn.KNeighborsClassifier(n_neighbors=k, metric=\"cosine\"))\n",
    "\n",
    "    # Initialize confusion matrices for training and testing across metrics\n",
    "    conf_train = [[[0 for i in range(10)]\n",
    "                   for i in range(10)] for i in range(3)]\n",
    "    conf_test = [[[0 for i in range(10)]\n",
    "                  for i in range(10)] for i in range(3)]\n",
    "\n",
    "    # For each metric, generate confustion matrix and print accuracy\n",
    "    for i in range(3):\n",
    "        # Train the model on training data\n",
    "        knn[i].fit(train_features, train_labels)\n",
    "\n",
    "        # Generate prediction data for the training set\n",
    "        pred_labels = knn[i].predict(train_features)\n",
    "        # Build confusion matrix for training set\n",
    "        for j in range(len(train_labels)):\n",
    "            conf_train[i][key[train_labels[j]]][key[pred_labels[j]]] += 1\n",
    "\n",
    "        # Generate prediction data for the test set\n",
    "        pred_labels = knn[i].predict(test_features)\n",
    "        # Build confusion matrix for test set\n",
    "        for j in range(len(test_labels)):\n",
    "            conf_test[i][key[test_labels[j]]][key[pred_labels[j]]] += 1\n",
    "\n",
    "        # Print out the training and testing confusion matrices\n",
    "        print_conf(conf_train[i], key)\n",
    "        print()\n",
    "        print_conf(conf_test[i], key)\n",
    "\n",
    "        # Print out % accuracy data\n",
    "        print(\"{:.1f}\\n\".format(knn[i].score(\n",
    "            test_features, test_labels) * 100))\n",
    "\n",
    "\n",
    "P8MNIST(mtrain[:5000], mtest[:1000], key_iris, 1)\n",
    "P8MNIST(mtrain[:5000], mtest[:1000], key_iris, 5)\n",
    "P8MNIST(mtrain[:5000], mtest[:1000], key_iris, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f2f02e1de8357b716a172ad62847c46880f415b272f7ede03f31fa82ee7994b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
