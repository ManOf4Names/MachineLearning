{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def split(D):\n",
    "    used = []\n",
    "    test = []\n",
    "    train = []\n",
    "    while len(test) < len(D) * 0.2:\n",
    "        i = rd.randint(0, len(D) - 1)\n",
    "        if i not in used:\n",
    "            used.append(i)\n",
    "            test.append(D.iloc[i].values.tolist())\n",
    "    for i in range(len(D)):\n",
    "        if i not in used:\n",
    "            train.append(D.iloc[i].values.tolist())\n",
    "    return (train, test)\n",
    "\n",
    "def dist_euc(p1, p2):\n",
    "    dist = 0\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        for i in range(len(p1)):\n",
    "            dist += (p1[i] - p2[i]) ** 2\n",
    "        return m.sqrt(dist)\n",
    "\n",
    "def dist_man(p1, p2):\n",
    "    dist = 0\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        for i in range(len(p1)):\n",
    "            dist += p1[i] + p2[i]\n",
    "        return dist\n",
    "\n",
    "\n",
    "def dist_cos(p1, p2):\n",
    "    dist = 0\n",
    "    if len(p1) != len(p2):\n",
    "        return 0\n",
    "    else:\n",
    "        l = len(p1)\n",
    "        dot = sum(i[0] * i[1] for i in zip(p1, p2))\n",
    "        return dot / (dist_euc([0] * l, p1) * dist_euc([0] * l, p2))\n",
    "\n",
    "train, test = split(pd.read_csv('iris.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6\\\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\\\n",
    "    1) Divide your data into random train and test sets\\\n",
    "    2) Training: for a specific label, find the average of each feature. (In case of mnist, this is pixel wise).  give your averages. In case of mint, give this as 10 images, one for each label\\\n",
    "    3) For each data point in the training set, find the closest from the average templates. Use Manhattan, Euclidean and Cosine distances (note that higher cosine value means lower distance). Give the point that label. \\\n",
    "    4) Do the same for the test set\\\n",
    "    5) Give the confusion matrix for training and test sets.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5.0, 3.4, 1.5, 0.2], [4.5, 2.3, 1.3, 0.3], [7.7, 2.6, 6.9, 2.3]], [[5.7, 2.8, 4.1, 1.3], [4.5, 2.3, 1.3, 0.3], [4.6, 3.6, 1.0, 0.2]], [[6.5, 3.0, 5.5, 1.8], [4.5, 2.3, 1.3, 0.3], [4.6, 3.6, 1.0, 0.2]]]\n",
      "[[[5.2, 3.4, 1.4, 0.2], [4.3, 3.0, 1.1, 0.1], [5.8, 2.8, 5.1, 2.4]], [[5.7, 2.9, 4.2, 1.3], [4.3, 3.0, 1.1, 0.1], [5.5, 4.2, 1.4, 0.2]], [[6.4, 2.8, 5.6, 2.1], [4.3, 3.0, 1.1, 0.1], [5.5, 4.2, 1.4, 0.2]]]\n"
     ]
    }
   ],
   "source": [
    "# Iris Algorithm\n",
    "def P6(train, test):\n",
    "    avg = {}\n",
    "    for row in train:\n",
    "        if row[4] not in avg:\n",
    "            avg[row[4]] = [row[0:4], 1]\n",
    "        else:\n",
    "            avg[row[4]][1] += 1\n",
    "            for i in range(len(row) - 1):\n",
    "                avg[row[4]][0][i] = avg[row[4]][0][i] + (row[i] - avg[row[4]][0][i]) / avg[row[4]][1]\n",
    "\n",
    "    train_vals = []\n",
    "    test_vals = []\n",
    "    for val in avg.values():\n",
    "        train_vals.append(find_min_dist(val[0], train))\n",
    "        test_vals.append(find_min_dist(val[0], test))\n",
    "\n",
    "    print(train_vals)\n",
    "    print(test_vals)\n",
    "\n",
    "def find_min_dist(avg, data):\n",
    "    dist = [m.inf, m.inf, m.inf]\n",
    "    keys = [[], [], []]\n",
    "    for i in data:\n",
    "        d = dist_euc(avg, i[:-1])\n",
    "        if d < dist[0]:\n",
    "            dist[0] = d\n",
    "            keys[0] = i[:-1]\n",
    "\n",
    "        d = dist_man(avg, i[:-1])\n",
    "        if d < dist[1]:\n",
    "            dist[1] = d\n",
    "            keys[1] = i[:-1]\n",
    "\n",
    "        d = dist_cos(avg, i[:-1])\n",
    "        if d < dist[2]:\n",
    "            dist[2] = d\n",
    "            keys[2] = i[:-1]\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "P6(train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\n",
    "    1) Divide your data into random train and test sets\n",
    "    2) Training: k-means clustering for k=1 to k=15 (i.e., run your algorithm 15 times). DO NOT USE LABLES. Give the centroids. In case of mint, give this as k images, one for each label. (you may use a library function)\n",
    "    3) Label a cluster using the majority label found in the cluster. \n",
    "    4) For each data point in the test set, find the closest from the centroid Use Manhattan, Euclidean and Cosine distances (note that higher cosine value means lower distance). Give the point that label. \n",
    "    5) Give the confusion matrix for training and test sets.\n",
    "    6) Form the elbow map - using %correct as a measure. (Test set accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8\n",
    "Consider the following algorithm. Run it on Iris.tab and Mnist datasets and report your results.\n",
    "    1) Divide your data into random train and test sets\n",
    "    2) Using kNN with Manhattan, Euclidean and cosine distances classify the training and test sets. Do this for k=1, k=5, k=10. (you may use a library function)\n",
    "    3) Give the confusion matrix for training and test sets.\n",
    "    4) Form the elbow map - using %correct as a measure. (Test set accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f2f02e1de8357b716a172ad62847c46880f415b272f7ede03f31fa82ee7994b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
