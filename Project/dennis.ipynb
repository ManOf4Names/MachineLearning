{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import sklearn.cluster as skc\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.feature_selection as skf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kicked Tensorflow to it's own cell so I can work on K-Means at work\n",
    "import tensorflow as tf\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Datasets/data.csv', header=0)\n",
    "genreKey = {\"blues\": 0,\n",
    "            \"classical\": 1,\n",
    "            \"country\": 2,\n",
    "            \"disco\": 3,\n",
    "            \"hiphop\": 4,\n",
    "            \"jazz\": 5,\n",
    "            \"metal\": 6,\n",
    "            \"pop\": 7,\n",
    "            \"reggae\": 8,\n",
    "            \"rock\": 9}\n",
    "\n",
    "def dimReduce(data, dims=10):\n",
    "    # Select Data\n",
    "    X = np.array(data.iloc[:, 1:29])\n",
    "    y = np.array(data.iloc[:, 29])\n",
    "    # Make the feature selection model with ANOVA F Measure\n",
    "    fs = skf.SelectKBest(score_func=skf.f_classif, k=dims)\n",
    "    # Run the data through selection to obtain final set\n",
    "    X_sel = fs.fit_transform(X, y)\n",
    "\n",
    "    return X_sel, y\n",
    "\n",
    "def prepData(data, key, split=0.2, dims= 10, doSplit=True):\n",
    "    # Reduce the dimensionality of the dataset to a set number of features\n",
    "    x, y = dimReduce(data, dims)\n",
    "    # Define the number of elements in the test set\n",
    "    splitRange = int(len(data) * split)\n",
    "    # Create and randomize an array representing data ordering\n",
    "    rand = [i for i in range(len(data))]\n",
    "    rd.shuffle(rand)\n",
    "    # Create Testing and training arrays\n",
    "    x_train = np.array([])\n",
    "    y_train = np.array([])\n",
    "    x_test = np.array([])\n",
    "    y_test = np.array([])\n",
    "\n",
    "    # Populate test arrays with a random split% of the whole set\n",
    "    for i in range(splitRange):\n",
    "        # Set the dimensions of the x_test array\n",
    "        if x_test.ndim == 1:\n",
    "            x_test = np.array([x[rand[i]]])\n",
    "        # Append further elements to the existing array\n",
    "        else:\n",
    "            x_test = np.append(x_test, [x[rand[i]]], 0)\n",
    "        # Add label in integer form\n",
    "        y_test = np.append(y_test, key[y[rand[i]]])\n",
    "            \n",
    "    for i in range(splitRange, len(data)):\n",
    "        # Set the dimensions of the x_train array\n",
    "        if x_train.ndim == 1:\n",
    "            x_train = np.array([x[rand[i]]])\n",
    "        # Append further elements to the existing array\n",
    "        else:\n",
    "            x_train = np.append(x_train, [x[rand[i]]], 0)\n",
    "        # Add label in integer form\n",
    "        y_train = np.append(y_train, key[y[rand[i]]])\n",
    "\n",
    "    if (doSplit):\n",
    "        return x_train, y_train, x_test, y_test\n",
    "    else:\n",
    "        return np.concatenate((x_train, x_test)), np.concatenate((y_train, y_test))\n",
    "\n",
    "def normalize(data):\n",
    "    # Array for the max value of each feature. Used in normalization\n",
    "    normax = []\n",
    "    # Array for the min value of each feature. Used to eliminate negatives\n",
    "    normin = []\n",
    "\n",
    "    # Populate max and min arrays\n",
    "    for i in range(data.shape[1]):\n",
    "        normin.append(min(data[:, i]))\n",
    "        normax.append(max(data[:, i]) - normin[i])\n",
    "\n",
    "    # Normalize each vector in the dataset\n",
    "    for i in data:\n",
    "        for j in range(len(i)):\n",
    "            i[j] = (i[j] - normin[j]) / normax[j]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Split the dataset into test and training sets.\n",
    "x_train, y_train, x_test, y_test = prepData(data, genreKey, dims=10)\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)\n",
    "\n",
    "# Unsplit Data for K-Means\n",
    "x, y = prepData(data, genreKey, doSplit=False)\n",
    "x = normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the raw data into labels and features for the testing & training sets\n",
    "rawData = data.values.tolist()\n",
    "\n",
    "#Shuffle the data so it's random\n",
    "rd.shuffle(rawData)\n",
    "\n",
    "#Get the labels and relevant features\n",
    "allLabels = [None] * len(rawData)\n",
    "allFeatures = [None] * len(rawData)\n",
    "\n",
    "#Label is column #30, index 29\n",
    "for i in range(len(allLabels)):\n",
    "    allLabels[i] = rawData[i][29]\n",
    "\n",
    "#Need to convert them to ints\n",
    "for i in range(len(allLabels)):\n",
    "    label = allLabels[i]\n",
    "    if(label == \"blues\"):\n",
    "        allLabels[i] = 0\n",
    "    elif(label == \"classical\"):\n",
    "        allLabels[i] = 1\n",
    "    elif(label == \"country\"):\n",
    "        allLabels[i] = 2\n",
    "    elif(label == \"disco\"):\n",
    "        allLabels[i] = 3\n",
    "    elif(label == \"hiphop\"):\n",
    "        allLabels[i] = 4\n",
    "    elif(label == \"jazz\"):\n",
    "        allLabels[i] = 5\n",
    "    elif(label == \"metal\"):\n",
    "        allLabels[i] = 6\n",
    "    elif(label == \"pop\"):\n",
    "        allLabels[i] = 7\n",
    "    elif(label == \"reggae\"):\n",
    "        allLabels[i] = 8\n",
    "    elif(label == \"rock\"):\n",
    "        allLabels[i] = 9\n",
    "\n",
    "#Make them the right dimensions\n",
    "allLabels = tf.keras.utils.to_categorical(allLabels)\n",
    "\n",
    "#We want the features at 3, 4, 5, 6, 7, 10, 11, 13, 15, & 17\n",
    "for i in range(len(allFeatures)):\n",
    "    rawItem = rawData[i][:]\n",
    "    item = rawItem[3:8]\n",
    "    item.append(rawItem[10])\n",
    "    item.append(rawItem[11])\n",
    "    item.append(rawItem[13])\n",
    "    item.append(rawItem[15])\n",
    "    item.append(rawItem[17])\n",
    "    allFeatures[i] = item\n",
    "\n",
    "#Make it the right type\n",
    "#allFeatures = np.expand_dims(allFeatures, axis=-1)\n",
    "\n",
    "#Determine how many elements the test set should contain\n",
    "testSize = round(len(rawData) * 0.2)\n",
    "\n",
    "#Split the sets\n",
    "testFeatures = np.asarray(allFeatures[:testSize])\n",
    "testLabels = np.asarray(allLabels[:testSize])\n",
    "trainFeatures = np.asarray(allFeatures[testSize:])\n",
    "trainLabels = np.asarray(allLabels[testSize:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "mlpModel = models.Sequential()\n",
    "mlpModel.add(layers.Dense(20, activation='relu', input_shape = (10,)))\n",
    "mlpModel.add(layers.Dense(60))\n",
    "mlpModel.add(layers.Dense(120))\n",
    "mlpModel.add(layers.Dense(30))\n",
    "mlpModel.add(layers.Dense(10))\n",
    "mlpModel.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "#mlpModel.build()\n",
    "#mlpModel.summary()\n",
    "history = mlpModel.fit(trainFeatures, trainLabels, epochs=10, shuffle=True, validation_data=(testFeatures, testLabels))\n",
    "mlpModel.fit(trainFeatures, trainLabels, epochs=10, shuffle=True, validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConf(acc, labels, clusters=10):\n",
    "    for i in range(clusters):\n",
    "        for j in acc[i]:\n",
    "            print(f\"{j}\\t\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    for i in range(clusters):\n",
    "        print(f\"Cluster value for label {i}: {labels[i]}\")\n",
    "\n",
    "\n",
    "def defLabels(acc):\n",
    "    labels = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9:[]}\n",
    "    for key, val in acc.items():\n",
    "        for i in range(len(val)):\n",
    "            if val[i] == max(val):\n",
    "                labels[key].append(i)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "# K-Means Classification\n",
    "def kMeans(x, y):\n",
    "    # Array for accuracy counts\n",
    "    acc = {}\n",
    "    kmeans = skc.KMeans(n_clusters=10, n_init=100, max_iter=1000, verbose=0).fit(x)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] not in acc:\n",
    "            acc[y[i]] = [0 for i in range(10)]\n",
    "        acc[y[i]][kmeans.labels_[i]] += 1\n",
    "    \n",
    "    return acc\n",
    "\n",
    "acc = kMeans(x, y)\n",
    "labels = defLabels(acc)\n",
    "printConf(acc, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ART\n",
    "#Create a new starting model\n",
    "def createModel():\n",
    "    newModel = models.Sequential()\n",
    "    newModel.add(layers.Dense(1, input_shape = (10,), kernel_initializer = \"random_normal\"))\n",
    "    newModel.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return newModel\n",
    "\n",
    "#Make a list of models to compete with each other\n",
    "modelList = []\n",
    "\n",
    "#Make the first model\n",
    "modelList.append(createModel())\n",
    "\n",
    "#Evaluate the first input, training the model on it\n",
    "centroids = []\n",
    "clusters = []\n",
    "output = modelList[0](np.asarray([trainFeatures[0]]))\n",
    "centroids.append(tf.keras.backend.get_value(output[0][0]))\n",
    "clusters.append([0])\n",
    "modelList[0].fit(np.asarray([trainFeatures[0]]), np.asarray([centroids[0]])) #Not sure if you can feed it a lable like this\n",
    "\n",
    "#Do this for every input\n",
    "vigilance = 500\n",
    "for i in range(1, len(trainFeatures)):\n",
    "    currentFeature = trainFeatures[i]\n",
    "    foundModel = False\n",
    "\n",
    "    #Find the best model for evaluating the current input\n",
    "    minDistance = float('inf')\n",
    "    bestModel = None\n",
    "    bestModelIndex = -1\n",
    "    for j in range(len(modelList)):\n",
    "        currentModel = modelList[j]\n",
    "\n",
    "        #Get an output\n",
    "        output = currentModel(np.asarray([currentFeature]))\n",
    "        output = tf.keras.backend.get_value(output[0][0])\n",
    "\n",
    "        #Check if the distance between the output and the centroid is the lowest found\n",
    "        if(abs(output - centroids[j]) < minDistance):\n",
    "            minDistance = abs(output - centroids[j])\n",
    "            bestModel = currentModel\n",
    "            bestModelIndex = j\n",
    "\n",
    "    #Check if the best model is close enough according to the vigilance\n",
    "    if(minDistance < vigilance):\n",
    "        #Update the centroid and cluster\n",
    "        centroids[bestModelIndex] = ((centroids[bestModelIndex] * len(clusters[bestModelIndex])) + output) / (len(clusters[bestModelIndex]) + 1)\n",
    "        clusters[bestModelIndex].append(i)\n",
    "\n",
    "        #Train the model on the input\n",
    "        bestModel.fit(np.asarray([currentFeature]), np.asarray([centroids[bestModelIndex]]))\n",
    "\n",
    "    #If it's not, make a new model for the unhandled value\n",
    "    else:\n",
    "        print(\"making new model\")\n",
    "\n",
    "        #Get the output from the new model\n",
    "        newModel = createModel()\n",
    "        output = newModel(np.asarray([currentFeature]))\n",
    "\n",
    "        #Make new entries for the model\n",
    "        modelList.append(newModel)\n",
    "        centroids.append(tf.keras.backend.get_value(output[0][0]))\n",
    "        clusters.append([i])\n",
    "\n",
    "        #Train the model on it's new centroid\n",
    "        newModel.fit(np.asarray([currentFeature]), np.asarray([tf.keras.backend.get_value(output[0][0])]))\n",
    "\n",
    "print(str(len(modelList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative ART\n",
    "#Make the model\n",
    "artModel = models.Sequential()\n",
    "artModel.add(layers.Dense(1, input_shape = (10,), kernel_initializer = \"random_normal\"))\n",
    "artModel.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "#Start the list of centroids & clusters\n",
    "centroids = []\n",
    "clusters = []\n",
    "\n",
    "#Loop through every input\n",
    "vigilance = 600\n",
    "for i in range(1, len(x_train)):\n",
    "    currentFeature = x_train[i]\n",
    "\n",
    "    #Get the output from the network \n",
    "    output = artModel(np.asarray([currentFeature]))\n",
    "    output = tf.keras.backend.get_value(output[0][0])\n",
    "\n",
    "    #Find the closest centroid\n",
    "    minDistance = float('inf')\n",
    "    bestClusterIndex = -1\n",
    "    for j in range(len(centroids)):\n",
    "        dist = abs(centroids[j] - output)\n",
    "        if(dist < minDistance):\n",
    "            minDistance = dist\n",
    "            bestClusterIndex = j\n",
    "\n",
    "    #Put the input in that cluster, if it's within vigilance distance\n",
    "    if(minDistance < vigilance):\n",
    "        centroids[bestClusterIndex] = ((centroids[bestClusterIndex] * len(clusters[bestClusterIndex])) + output) / (len(clusters[bestClusterIndex]) + 1)\n",
    "        clusters[bestClusterIndex].append(i)\n",
    "\n",
    "        #Also train the model on that centroid & input\n",
    "        artModel.fit(np.asarray([currentFeature]), np.asarray([centroids[bestClusterIndex]]))\n",
    "\n",
    "    #If it's not within the vigilance distance, make a new cluster\n",
    "    else:\n",
    "        print(\"Making new cluster\")\n",
    "        centroids.append(output)\n",
    "        clusters.append([i])\n",
    "        artModel.fit(np.asarray([currentFeature]), np.asarray([output]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ART printout\n",
    "print(\"Printing \" + str(len(clusters)) + \" clusters\")\n",
    "\n",
    "#Initialize the cluster count list\n",
    "clusterCounts = [[0 for i in range(10)] for i in range(len(clusters))]\n",
    "\n",
    "#For each cluster, count the occurances of each label\n",
    "for i in range(len(clusters)):\n",
    "    for labelIndex in clusters[i]:\n",
    "        clusterCounts[i][np.argmax(trainLabels[labelIndex])] += 1\n",
    "\n",
    "for count in clusterCounts:\n",
    "    print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f2f02e1de8357b716a172ad62847c46880f415b272f7ede03f31fa82ee7994b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
