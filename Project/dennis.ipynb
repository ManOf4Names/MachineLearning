{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import sklearn.cluster as skc\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.feature_selection as skf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kicked Tensorflow to it's own cell so I can work on K-Means at work\n",
    "import tensorflow as tf\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Datasets/data.csv', header=0)\n",
    "genreKey = {\"blues\": 0,\n",
    "            \"classical\": 1,\n",
    "            \"country\": 2,\n",
    "            \"disco\": 3,\n",
    "            \"hiphop\": 4,\n",
    "            \"jazz\": 5,\n",
    "            \"metal\": 6,\n",
    "            \"pop\": 7,\n",
    "            \"reggae\": 8,\n",
    "            \"rock\": 9}\n",
    "\n",
    "def dimReduce(data, dims=10):\n",
    "    # Select Data\n",
    "    X = np.array(data.iloc[:, 1:29])\n",
    "    y = np.array(data.iloc[:, 29])\n",
    "    # Make the feature selection model with ANOVA F Measure\n",
    "    fs = skf.SelectKBest(score_func=skf.f_classif, k=dims)\n",
    "    # Run the data through selection to obtain final set\n",
    "    X_sel = fs.fit_transform(X, y)\n",
    "\n",
    "    return X_sel, y\n",
    "\n",
    "def prepData(data, key, split=0.2, dims= 10, doSplit=True):\n",
    "    # Reduce the dimensionality of the dataset to a set number of features\n",
    "    x, y = dimReduce(data, dims)\n",
    "    # Define the number of elements in the test set\n",
    "    splitRange = int(len(data) * split)\n",
    "    # Create and randomize an array representing data ordering\n",
    "    rand = [i for i in range(len(data))]\n",
    "    rd.shuffle(rand)\n",
    "    # Create Testing and training arrays\n",
    "    x_train = np.array([])\n",
    "    y_train = np.array([])\n",
    "    x_test = np.array([])\n",
    "    y_test = np.array([])\n",
    "\n",
    "    # Populate test arrays with a random split% of the whole set\n",
    "    for i in range(splitRange):\n",
    "        # Set the dimensions of the x_test array\n",
    "        if x_test.ndim == 1:\n",
    "            x_test = np.array([x[rand[i]]])\n",
    "        # Append further elements to the existing array\n",
    "        else:\n",
    "            x_test = np.append(x_test, [x[rand[i]]], 0)\n",
    "        # Add label in integer form\n",
    "        y_test = np.append(y_test, key[y[rand[i]]])\n",
    "            \n",
    "    for i in range(splitRange, len(data)):\n",
    "        # Set the dimensions of the x_train array\n",
    "        if x_train.ndim == 1:\n",
    "            x_train = np.array([x[rand[i]]])\n",
    "        # Append further elements to the existing array\n",
    "        else:\n",
    "            x_train = np.append(x_train, [x[rand[i]]], 0)\n",
    "        # Add label in integer form\n",
    "        y_train = np.append(y_train, key[y[rand[i]]])\n",
    "\n",
    "    if (doSplit):\n",
    "        return x_train, y_train, x_test, y_test\n",
    "    else:\n",
    "        return np.concatenate((x_train, x_test)), np.concatenate((y_train, y_test))\n",
    "\n",
    "def normalize(data):\n",
    "    # Array for the max value of each feature. Used in normalization\n",
    "    normax = []\n",
    "    # Array for the min value of each feature. Used to eliminate negatives\n",
    "    normin = []\n",
    "\n",
    "    # Populate max and min arrays\n",
    "    for i in range(data.shape[1]):\n",
    "        normin.append(min(data[:, i]))\n",
    "        normax.append(max(data[:, i]) - normin[i])\n",
    "\n",
    "    # Normalize each vector in the dataset\n",
    "    for i in data:\n",
    "        for j in range(len(i)):\n",
    "            i[j] = (i[j] - normin[j]) / normax[j]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Split the dataset into test and training sets.\n",
    "x_train, y_train, x_test, y_test = prepData(data, genreKey, dims=10)\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)\n",
    "\n",
    "# Unsplit Data for K-Means\n",
    "x, y = prepData(data, genreKey, doSplit=False)\n",
    "x = normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 1, 4, 2, 4, 2, 3, 0, 0, 0, 1, 3, 3, 3, 2, 3, 3, 4, 2, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "upper = data.shape[1] - 1\n",
    "corrList = [0 for i in range(upper-1)]\n",
    "for i in range(1, upper):\n",
    "    for j in range(i + 1, upper):\n",
    "        corr = sp.stats.pearsonr(data.iloc[:, i], data.iloc[:, j])\n",
    "        if corr[0] >= 0.7:\n",
    "            corrList[i] += 1\n",
    "            corrList[j] += 1\n",
    "print(corrList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 1s 8ms/step - loss: 2.1731 - accuracy: 0.1988 - val_loss: 2.0472 - val_accuracy: 0.2700\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9349 - accuracy: 0.3100 - val_loss: 1.8699 - val_accuracy: 0.3100\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7975 - accuracy: 0.3313 - val_loss: 1.7712 - val_accuracy: 0.3700\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7011 - accuracy: 0.3613 - val_loss: 1.7013 - val_accuracy: 0.3750\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6255 - accuracy: 0.3837 - val_loss: 1.6855 - val_accuracy: 0.3500\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5816 - accuracy: 0.3913 - val_loss: 1.6764 - val_accuracy: 0.3600\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5294 - accuracy: 0.4363 - val_loss: 1.6399 - val_accuracy: 0.3800\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5089 - accuracy: 0.4200 - val_loss: 1.7124 - val_accuracy: 0.4050\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4912 - accuracy: 0.4325 - val_loss: 1.5620 - val_accuracy: 0.4350\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4887 - accuracy: 0.4288 - val_loss: 1.6075 - val_accuracy: 0.3950\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "#Reformat the labels to work with this model\n",
    "mlp_train_labels = tf.keras.utils.to_categorical(y_train)\n",
    "mlp_test_labels = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "#Set up & run the network\n",
    "mlpModel = models.Sequential()\n",
    "mlpModel.add(layers.Dense(20, activation='relu', input_shape = np.shape(x_train[0])))\n",
    "mlpModel.add(layers.Dense(60))\n",
    "mlpModel.add(layers.Dense(120))\n",
    "mlpModel.add(layers.Dense(30))\n",
    "mlpModel.add(layers.Dense(10))\n",
    "mlpModel.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "#mlpModel.build()\n",
    "#mlpModel.summary()\n",
    "history = mlpModel.fit(x_train, mlp_train_labels, epochs=10, shuffle=True, validation_data=(x_test, mlp_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConf(acc, labels, clusters=10):\n",
    "    for i in range(clusters):\n",
    "        for j in acc[i]:\n",
    "            print(f\"{j}\\t\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    for i in range(clusters):\n",
    "        print(f\"Cluster value for label {i}: {labels[i]}\")\n",
    "\n",
    "\n",
    "def defLabels(acc):\n",
    "    labels = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9:[]}\n",
    "    for key, val in acc.items():\n",
    "        for i in range(len(val)):\n",
    "            if val[i] == max(val):\n",
    "                labels[key].append(i)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "# K-Means Classification\n",
    "def kMeans(x, y):\n",
    "    # Array for accuracy counts\n",
    "    acc = {}\n",
    "    kmeans = skc.KMeans(n_clusters=10, n_init=100, max_iter=1000, verbose=0).fit(x)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] not in acc:\n",
    "            acc[y[i]] = [0 for i in range(10)]\n",
    "        acc[y[i]][kmeans.labels_[i]] += 1\n",
    "    \n",
    "    return acc\n",
    "\n",
    "acc = kMeans(x, y)\n",
    "labels = defLabels(acc)\n",
    "printConf(acc, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making new model\n",
      "making new model\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#ART\n",
    "#This does not work very well\n",
    "\n",
    "#Create a new starting model\n",
    "def createModel():\n",
    "    newModel = models.Sequential()\n",
    "    newModel.add(layers.Dense(1, input_shape = np.shape(x_train[0]), kernel_initializer = \"random_normal\"))\n",
    "    newModel.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return newModel\n",
    "\n",
    "#Make a list of models to compete with each other\n",
    "modelList = []\n",
    "\n",
    "#Make the first model\n",
    "modelList.append(createModel())\n",
    "\n",
    "#Evaluate the first input, training the model on it\n",
    "centroids = []\n",
    "clusters = []\n",
    "output = modelList[0](np.asarray([x_train[0]]))\n",
    "centroids.append(tf.keras.backend.get_value(output[0][0]))\n",
    "clusters.append([0])\n",
    "modelList[0].fit(np.asarray([x_train[0]]), np.asarray([centroids[0]]), verbose = 0)\n",
    "\n",
    "#Do this for every input\n",
    "vigilance = 0.4\n",
    "for i in range(1, len(x_train)):\n",
    "    currentFeature = x_train[i]\n",
    "    foundModel = False\n",
    "\n",
    "    #Find the best model for evaluating the current input\n",
    "    minDistance = float('inf')\n",
    "    bestModel = None\n",
    "    bestModelIndex = -1\n",
    "    for j in range(len(modelList)):\n",
    "        currentModel = modelList[j]\n",
    "\n",
    "        #Get an output\n",
    "        output = currentModel(np.asarray([currentFeature]))\n",
    "        output = tf.keras.backend.get_value(output[0][0])\n",
    "\n",
    "        #Check if the distance between the output and the centroid is the lowest found\n",
    "        if(abs(output - centroids[j]) < minDistance):\n",
    "            minDistance = abs(output - centroids[j])\n",
    "            bestModel = currentModel\n",
    "            bestModelIndex = j\n",
    "\n",
    "    #Check if the best model is close enough according to the vigilance\n",
    "    if(minDistance < vigilance):\n",
    "        #Update the centroid and cluster\n",
    "        centroids[bestModelIndex] = ((centroids[bestModelIndex] * len(clusters[bestModelIndex])) + output) / (len(clusters[bestModelIndex]) + 1)\n",
    "        clusters[bestModelIndex].append(i)\n",
    "\n",
    "        #Train the model on the input\n",
    "        bestModel.fit(np.asarray([currentFeature]), np.asarray([centroids[bestModelIndex]]), verbose = 0)\n",
    "\n",
    "    #If it's not, make a new model for the unhandled value\n",
    "    else:\n",
    "        print(\"making new model\")\n",
    "\n",
    "        #Get the output from the new model\n",
    "        newModel = createModel()\n",
    "        output = newModel(np.asarray([currentFeature]))\n",
    "\n",
    "        #Make new entries for the model\n",
    "        modelList.append(newModel)\n",
    "        centroids.append(tf.keras.backend.get_value(output[0][0]))\n",
    "        clusters.append([i])\n",
    "\n",
    "        #Train the model on it's new centroid\n",
    "        newModel.fit(np.asarray([currentFeature]), np.asarray([tf.keras.backend.get_value(output[0][0])]), verbose = 0)\n",
    "\n",
    "print(str(len(modelList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed round 0\n",
      "Completed round 1\n",
      "Completed round 2\n",
      "Completed round 3\n",
      "Completed round 4\n",
      "Completed round 5\n",
      "Completed round 6\n",
      "Completed round 7\n",
      "Completed round 8\n",
      "Completed round 9\n",
      "Completed round 10\n",
      "Completed round 11\n",
      "Completed round 12\n",
      "Completed round 13\n",
      "Completed round 14\n",
      "Completed round 15\n",
      "Completed round 16\n",
      "Completed round 17\n",
      "Completed round 18\n",
      "Completed round 19\n",
      "Completed round 20\n",
      "Completed round 21\n",
      "Completed round 22\n",
      "Completed round 23\n",
      "Completed round 24\n",
      "Completed round 25\n",
      "Completed round 26\n",
      "Completed round 27\n",
      "Completed round 28\n",
      "Completed round 29\n",
      "Completed round 30\n",
      "Completed round 31\n",
      "Completed round 32\n",
      "Completed round 33\n",
      "Completed round 34\n",
      "Completed round 35\n",
      "Completed round 36\n",
      "Completed round 37\n",
      "Completed round 38\n",
      "Completed round 39\n",
      "Completed round 40\n",
      "Completed round 41\n",
      "Completed round 42\n",
      "Completed round 43\n",
      "Completed round 44\n",
      "Completed round 45\n",
      "Completed round 46\n",
      "Completed round 47\n",
      "Completed round 48\n",
      "Completed round 49\n",
      "Completed round 50\n",
      "Completed round 51\n",
      "Completed round 52\n",
      "Completed round 53\n",
      "Completed round 54\n",
      "Completed round 55\n",
      "Completed round 56\n",
      "Completed round 57\n",
      "Completed round 58\n",
      "Completed round 59\n",
      "Completed round 60\n",
      "Completed round 61\n",
      "Completed round 62\n",
      "Completed round 63\n",
      "Completed round 64\n",
      "Completed round 65\n",
      "Completed round 66\n",
      "Completed round 67\n",
      "Completed round 68\n",
      "Completed round 69\n",
      "Completed round 70\n",
      "Completed round 71\n",
      "Completed round 72\n",
      "Completed round 73\n",
      "Completed round 74\n",
      "Completed round 75\n",
      "Completed round 76\n",
      "Completed round 77\n",
      "Completed round 78\n",
      "Completed round 79\n",
      "Completed round 80\n",
      "Completed round 81\n",
      "Completed round 82\n",
      "Completed round 83\n",
      "Completed round 84\n",
      "Completed round 85\n",
      "Completed round 86\n",
      "Completed round 87\n",
      "Completed round 88\n",
      "Completed round 89\n",
      "Completed round 90\n",
      "Completed round 91\n",
      "Completed round 92\n",
      "Completed round 93\n",
      "Completed round 94\n",
      "Completed round 95\n",
      "Completed round 96\n",
      "Completed round 97\n",
      "Completed round 98\n",
      "Completed round 99\n"
     ]
    }
   ],
   "source": [
    "#Alternative ART\n",
    "#Works ever so slightly better\n",
    "\n",
    "#Make the model\n",
    "artModel = models.Sequential()\n",
    "artModel.add(layers.Dense(1, input_shape = np.shape(x_train[0]), kernel_initializer = \"random_normal\"))\n",
    "artModel.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "for roundNum in range(100):\n",
    "\n",
    "    #Start the list of centroids & clusters\n",
    "    centroids = []\n",
    "    clusters = []\n",
    "\n",
    "    #Loop through every input\n",
    "    vigilance = 0.4 * (roundNum + 1)\n",
    "    for i in range(1, len(x_train)):\n",
    "        currentFeature = x_train[i]\n",
    "\n",
    "        #Get the output from the network \n",
    "        output = artModel(np.asarray([currentFeature]))\n",
    "        output = tf.keras.backend.get_value(output[0][0])\n",
    "\n",
    "        #Find the closest centroid\n",
    "        minDistance = float('inf')\n",
    "        bestClusterIndex = -1\n",
    "        for j in range(len(centroids)):\n",
    "            dist = abs(centroids[j] - output)\n",
    "            if(dist < minDistance):\n",
    "                minDistance = dist\n",
    "                bestClusterIndex = j\n",
    "\n",
    "        #Put the input in that cluster, if it's within vigilance distance\n",
    "        if(minDistance < vigilance):\n",
    "            centroids[bestClusterIndex] = ((centroids[bestClusterIndex] * len(clusters[bestClusterIndex])) + output) / (len(clusters[bestClusterIndex]) + 1)\n",
    "            clusters[bestClusterIndex].append(i)\n",
    "\n",
    "            #Also train the model on that centroid & input\n",
    "            artModel.fit(np.asarray([currentFeature]), np.asarray([centroids[bestClusterIndex]]), verbose = 0)\n",
    "\n",
    "        #If it's not within the vigilance distance, make a new cluster\n",
    "        else:\n",
    "            centroids.append(output)\n",
    "            clusters.append([i])\n",
    "            artModel.fit(np.asarray([currentFeature]), np.asarray([output]), verbose = 0)\n",
    "\n",
    "    print(\"Completed round \" + str(roundNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing 8 clusters\n",
      "[10, 0, 26, 13, 19, 21, 0, 15, 42, 25]\n",
      "[17, 0, 5, 20, 23, 0, 39, 22, 4, 13]\n",
      "[6, 31, 9, 1, 1, 23, 0, 2, 1, 0]\n",
      "[19, 0, 17, 43, 34, 9, 19, 39, 25, 36]\n",
      "[0, 0, 0, 1, 5, 0, 18, 1, 1, 0]\n",
      "[0, 28, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[27, 12, 25, 0, 1, 25, 0, 6, 6, 6]\n",
      "[0, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#ART printout\n",
    "print(\"Printing \" + str(len(clusters)) + \" clusters\")\n",
    "\n",
    "#Initialize the cluster count list\n",
    "clusterCounts = [[0 for i in range(10)] for i in range(len(clusters))]\n",
    "\n",
    "#For each cluster, count the occurances of each label\n",
    "for i in range(len(clusters)):\n",
    "    for labelIndex in clusters[i]:\n",
    "        clusterCounts[i][round(y_train[labelIndex])] += 1\n",
    "\n",
    "for count in clusterCounts:\n",
    "    print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f2f02e1de8357b716a172ad62847c46880f415b272f7ede03f31fa82ee7994b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
